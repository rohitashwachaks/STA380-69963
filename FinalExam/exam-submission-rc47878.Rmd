---
title: "Untitled"
author: "Rohitashwa Chakraborty"
date: "30/07/2021"
output:
  pdf_document: default
---
# EXERCISE 4.10:

```{r 4.10.setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

knitr::opts_chunk$set(set.seed(1))

rm(list = ls())
require(ISLR)
attach(Weekly)
require(caret)
require(class)
```

## _4.10.a_

```{r 4.10.a}
summary(Weekly)
pairs(Weekly[,-9])
```

Positive Correlation between `Year` and `Volume` observed.

## _4.10.b_

```{r 4.10.b}
mdl <- glm(Direction~., data=Weekly[,c(2:7,9)], family=binomial)
summary(mdl)
```

To check if a parameter is significant or not, we must check for its __P-Vaue__.

From the Summary, only `Lag2` has a __P-Value__ < 0.05.
Thus, only `Lag2` is statistically significant.

## _4.10.c_

```{r 4.10.c}
preds <- as.factor(ifelse(mdl$fitted > 0.5, "Up", "Down"))
cm <- confusionMatrix(preds, Direction, positive = "Up")
cm$table
cat("Accuracy            : ",paste(round(cm$overall["Accuracy"], digits = 4)*100),"%")
cat("Recall/Sensitivity  : ",paste(round(cm$byClass["Recall"], digits = 4)*100),"%")
cat("Precision           : ",paste(round(cm$byClass["Precision"], digits = 4)*100),"%")
cat("Specificity         : ",paste(round(cm$byClass["Specificity"], digits = 4)*100),"%")
cat("Up Prediction Rate  : ",paste(round(cm$byClass["Pos Pred Value"], digits = 4)*100),"%")
cat("Down Prediction Rate: ",paste(round(cm$byClass["Neg Pred Value"], digits = 4)*100),"%")
```
48 _"Up"_ were mistaken for _"Down"_.
430 _"Down"_ were mistaken for _"Up"_.
54 _"Down"_+ 557 _"Up"_ were predicted accurately .
Model is has higher accuracy when the prediction is _"Up"_

These results were obtained from the same set of observations the model was trained upon.
Therefore, it is highly likely that the results would prove to be _overly optimistic_ 
when tested on a new set of data.

## _4.10.d_

```{r 4.10.d}
filtered_years <- Weekly$Year %in% (1990:2008)
train <- Weekly[filtered_years,]
test <- Weekly[!filtered_years,]
mdl <- glm(Direction~Lag2, data=train, family=binomial)
preds <- predict(mdl, test, type="response")
preds <- as.factor(ifelse(preds > 0.5, "Up", "Down"))
cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
cm$table
cat("[Logistic Regression] Overall Fraction of Correct Predictions (Accuracy) : ",
    paste(round(cm$overall["Accuracy"], digits = 2)))
```

## _4.10.g_

```{r 4.10.g}
preds <- knn(as.matrix(train$Lag2), as.matrix(test$Lag2), train$Direction, k=1)
cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
cm$table
cat("[KNN (k = 1)] Overall Fraction of Correct Predictions (Accuracy) : ",
    paste(round(cm$overall["Accuracy"], digits = 2)))
```

## _4.10.h_

Considering **only Accuracy** as our metric, we can conclude that _Logistic Regression_
outperforms _KNN (with k = 1)_ 

## _4.10.i_

>Experimenting with different KNN models:

```{r 4.10.i.1}
cat("Predictors: Lag2\n")
for(neighbourhood in seq(30,400, by = 100)){
  preds <- knn(as.matrix(train$Lag2), 
               as.matrix(test$Lag2), 
               train$Direction, 
               k=neighbourhood)
  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  cat("[KNN (k = ",paste(neighbourhood),")] Accuracy : ",
    paste(round(cm$overall["Accuracy"], digits = 2)),"\n")
}

cat("\nPredictors: Lag2, Lag1\n")
for(neighbourhood in seq(30,400, by = 100)){
  preds <- knn(as.matrix(train$Lag2,train$Lag1),
               as.matrix(test$Lag2,test$Lag1),
               train$Direction, k=neighbourhood)

  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  cat("[KNN (k = ",paste(neighbourhood),")] Accuracy : ",
    paste(round(cm$overall["Accuracy"], digits = 2)),"\n")
}

cat("\nPredictors: Lag2^2\n")
for(neighbourhood in seq(30,400, by = 100)){
  preds <- knn(as.matrix(I(train$Lag2^2)), 
               as.matrix(I(test$Lag2^2)), 
               train$Direction, 
               k=neighbourhood)
  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  cat("[KNN (k = ",paste(neighbourhood),")] Accuracy : ",
    paste(round(cm$overall["Accuracy"], digits = 2)),"\n")
}

cat("\nPredictors: Lag2*Lag1\n")
for(neighbourhood in seq(30,400, by = 100)){
  preds <- knn(as.matrix(train$Lag2*train$Lag1),
               as.matrix(test$Lag2*test$Lag1),
               train$Direction, k=neighbourhood)

  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  cat("[KNN (k = ",paste(neighbourhood),")] Accuracy : ",
    paste(round(cm$overall["Accuracy"], digits = 2)),"\n")
}

cat("\nPredictors: All\n")
for(neighbourhood in seq(30,400, by = 100)){
  preds <- knn(as.matrix(train[,-9]),
               as.matrix(test[,-9]),
               train$Direction, k=neighbourhood)

  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  cat("[KNN (k = ",paste(neighbourhood),")] Accuracy : ",
    paste(round(cm$overall["Accuracy"], digits = 2)),"\n")
}
```

Considering only **Accuracy**, we can conclude that the following models perform the best:

**K= 30, Predictors: All Predictors**

```{r4.10.i.2}
preds <- knn(as.matrix(train[,-9]),
               as.matrix(test[,-9]),
               train$Direction, k=30)
cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
cat("Confusion Matrix for K= 130, Predictors: All\n")
cm$table
```

>Experimenting with different Logistic Regression Models:

```{r 4.10.i.3}
cat("Logistic Regression\n")
for(predictor in c("Lag2","Lag2+Lag1","Lag2*Lag1","I(Lag2^2)",".")){
  mdl <- glm(paste("Direction~",predictor), 
             data=train, family=binomial)
  preds <- predict(mdl, test, type="response")
  preds <- as.factor(ifelse(preds > 0.5, "Up", "Down"))
  cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
  if(predictor == "."){
    cat("\n[Predictors: All] Accuracy : ", paste(round(cm$overall["Accuracy"], digits = 2)))
  }
  else{
    cat("\n[Predictors:",predictor,"] Accuracy : ", paste(round(cm$overall["Accuracy"], digits = 2)))
  }
}
```
Considering Accuracy, It seems Using **All the Parameters** gives by far the most
accurate model with a **100% Accuracy**.

```{r 4.10.i.4}
mdl <- glm(Direction~., 
             data=train, family=binomial)
preds <- predict(mdl, test, type="response")
preds <- as.factor(ifelse(preds > 0.5, "Up", "Down"))
cm <- confusionMatrix(preds, Direction[!filtered_years], positive = "Up")
cat("Confusion Matrix for Linear Regression Model with All Predictors:\n")
cm$table
```

_NOTE: This is not surprising because one of the predictors the model trains upon is **Today**._
_This predictor seems to have a distinct linear boundary when plotted against **Direction**_
```{r 4.10.i.5, fig.align='center', fig.height = 5, fig.width = 5}
plot(Weekly$Direction,Weekly$Today,
     xlab="Direction", ylab="Today", main="Spread of Today v/s Direction")
```


***
<a id="4.10"></a>

# EXERCISE 6.9:

```{r 6.9.setup, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

knitr::opts_chunk$set(set.seed(1))

rm(list = ls())
require(MASS)
require(ISLR)
require(leaps)
require(glmnet)
require(pls)
attach(College)
set.seed(1)

College$Private <- as.factor(College$Private)
#Scaling the variables
College[c(-1,-2)] <- as.data.frame(scale(College[c(-1,-2)]))
```

## _6.9.a_

Creating a **80-20** split between *Train* and *Test* set
```{r 6.9.a}
n <- dim(College)[1]
tr <- sample(c(1:n), round(0.8*n, digits = 0))
train <- College[tr,]
test <- College[-tr,]
cat("Length of College Dataset:",paste(dim(College)[1]),"\n")
cat("Length of Train Dataset  :",paste(dim(train)[1]),"\n")
cat("Length of Test Dataset   :",paste(dim(test)[1]),"\n")
```
## _6.9.b_
The test error on applying Linear Regression on a Model with All Parameters is:
```{r 6.9.b}
mdl <- lm(Apps~., data=train)

preds <- predict(mdl,newdata = test)
linreg_error <- mean((preds-test$Apps)^2)
cat(paste(round(linreg_error, digits = 3)))
```
## _6.9.c_
```{r 6.9.c.1}
inp_train <- model.matrix(Apps~., data=train)[,-1]
inp_test <- model.matrix(Apps~., data=test)[,-1]

mdl <- cv.glmnet(inp_train, train$Apps, alpha=0)
lambda <- mdl$lambda.min  # optimal lambda
cat("Optimal Lambda, by 10-fold cross-validation is:",paste(lambda))
```
```{r 6.9.c.2, fig.align='center', fig.height = 5, fig.width = 5}
plot(mdl)
```
The test error on applying **Ridge-Regression** on a Model with All Parameters is:

```{r 6.9.c.3}
ridge_preds <- predict(mdl, s=lambda, newx=inp_test)
ridge_err <- mean((ridge_preds - test$Apps)^2)
# sd((ridge_preds - test$Apps)^2)
cat(paste(round(ridge_err, digits = 3)))
```
## _6.9.d_
```{r 6.9.d.1}
mdl <- cv.glmnet(inp_train, train$Apps, alpha=1)
lambda <- mdl$lambda.min  # optimal lambda
cat("Optimal Lambda, by 10-fold cross-validation is:",paste(lambda))
```
```{r 6.9.d.2, fig.align='center', fig.height = 5, fig.width = 5}
plot(mdl)
```
The test error on applying **Lasso-Regression** on a Model with All Parameters is:

```{r 6.9.d.3}
preds <- predict(mdl, s=lambda, newx=inp_test)
lasso_err <- mean((preds - test$Apps)^2)

cat(paste(round(lasso_err, digits = 3)))
```
Coefficients of Predictors using the Lasso-Regression method are:
```{r 6.9.d.4}
preds <- predict(mdl, type="coefficients", s=lambda, newx=inp_test)[-1,]
preds
```
Thus, Non-Zero Coefficient Estimate Predictors are:
```{r 6.9.d.5}
colnames(t(preds))[which(preds != 0)]
```

## _6.9.e_
```{r 6.9.e.1}
mdl <- pcr(Apps~., data=train, scale=TRUE, validation="CV")
validationplot(mdl, val.type="MSEP")
summary(mdl)
cat("Minimum CV at ")
```
Minimum CV at **M = 17**. Thus, using _predict(...,ncomp=17,...)_

The test error on applying **Principal Component Regression** on a Model with All Parameters is:
```{r 6.9.e.2}
preds <- predict(mdl, test, ncomp=17)
pcr_err <- mean((test$Apps - preds)^2)

cat(paste(round(pcr_err, digits = 3)))
```
## _6.9.f_
```{r 6.9.f.1}
mdl <- plsr(Apps~., data=train, scale=TRUE, validation="CV")
validationplot(mdl, val.type="MSEP")
summary(mdl)
```
Minimum CV at **M = 13**. Thus, using _predict(...,ncomp=13,...)_

The test error on applying **Partial Least Squares Regression** on a Model with All Parameters is:
```{r 6.9.f.2}
preds <- predict(mdl, test, ncomp=13)
plsr_err <- mean((test$Apps - preds)^2)

cat(paste(round(plsr_err, digits = 3)))
```
## _6.9.g_
```{r 6.9.g.1, fig.align='center', fig.height = 5, fig.width = 5}
all_errs <- c(linreg_error, ridge_err, lasso_err, pcr_err, plsr_err)
names(all_errs) <- c("linear", "ridge", "lasso", "pcr", "pls")
barplot(all_errs, xlab = "Type of Regression", ylab = "MSE", 
        main = "Comparison of Regression Fit")
```

Most of the regression methods _("Linear", "Ridge", "Lasso", "PCR", "PLS")_ have approximately the same amount of error.

The **Ridge Regression** outperforms others by a slight margin.
Its **Test MSE** is: 1447148.005 _(standard deviation = 5319499)_
```{r 6.9.g.2, fig.align='center', fig.height = 5, fig.width = 5}
plot(test$Apps, xlab= "Actual Admits", 
     ridge_preds, ylab = "Predicted Admits",
     main = "Prediction Error in Ridge Regression",
     xlim = c(0,20000), ylim = c(0,20000))
abline(1,1, col="red")
```
***
<a id="6.9"></a>

# EXERCISE 6.11:

```{r 6.11.setup, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

knitr::opts_chunk$set(set.seed(7))

rm(list = ls())
require(leaps)   # forward and backward selection
require(glmnet)  # ridge and lasso
require(MASS)    # Boston data set
attach(Boston)

```

## _6.11.a_

Creating a **80-20** split between *Train* and *Test* set
```{r 6.11.a.1}
tr <- sample(c(1:nrow(Boston)), round(0.8*nrow(Boston), digits = 0))
train <- Boston[tr,]
test <- Boston[-tr,]

inp_train <- model.matrix(crim~., data=train)[,-1]
inp_test <- model.matrix(crim~., data=test)[,-1]

cat("Length of Boston Dataset:",paste(nrow(Boston)),"\n")
cat("Length of Train Dataset :",paste(nrow(inp_train)),"\n")
cat("Length of Test Dataset  :",paste(nrow(inp_test)))
```
> **Ridge Regression**

```{r 6.11.a.ridge}
# ridge regression model
mdl <- cv.glmnet(inp_train, train$crim, alpha=0)
plot(mdl)
lambda <- mdl$lambda.min
cat("Optimal Lambda, by 10-fold cross-validation is:",paste(round(lambda, digits = 2)))
preds <- predict(mdl, s=lambda, newx=inp_test)
ridge_err <- mean((test$crim - preds)^2)
cat("\nTest Error of Ridge Regression:",paste(round(ridge_err, digits = 2)))
# predict(mdl, s=lambda, type="coefficients")
```
> **Lasso Regression**

```{r 6.11.a.lasso}
# lasso regression model
mdl <- cv.glmnet(inp_train, train$crim, alpha=1)
plot(mdl)
lambda <- mdl$lambda.min
cat("Optimal Lambda, by 10-fold cross-validation is:",paste(round(lambda, digits = 2)))
preds <- predict(mdl, s=lambda, newx=inp_test)
lasso_err <- mean((test$crim - preds)^2)
cat("\nTest Error of Lasso Regression:",paste(round(lasso_err, digits = 2)))
# predict(mdl, s=lambda, type="coefficients")
```
<!-- > **Principal Component Regression** -->

<!-- ```{r 6.11.a.pcr} -->
<!-- # principal component regression model -->
<!-- mdl <- cv.glmnet(inp_train, train$crim, alpha=1) -->
<!-- plot(mdl) -->
<!-- lambda <- mdl$lambda.min -->
<!-- cat("Optimal Lambda, by 10-fold cross-validation is:",paste(round(lambda, digits = 2))) -->
<!-- preds <- predict(mdl, s=lambda, newx=inp_test) -->
<!-- lasso_err <- mean((test$crim - preds)^2) -->
<!-- cat("\nTest Error of Lasso Regression:",paste(round(lasso_err, digits = 2))) -->
<!-- # predict(mdl, s=lambda, type="coefficients") -->
<!-- ``` -->
<!-- > **Partial Least Square Regression** -->

<!-- ```{r 6.11.a.plsr} -->
<!-- # partial least square regression model -->
<!-- mdl <- cv.glmnet(inp_train, train$crim, alpha=1) -->
<!-- plot(mdl) -->
<!-- lambda <- mdl$lambda.min -->
<!-- cat("Optimal Lambda, by 10-fold cross-validation is:",paste(round(lambda, digits = 2))) -->
<!-- preds <- predict(mdl, s=lambda, newx=inp_test) -->
<!-- lasso_err <- mean((test$crim - preds)^2) -->
<!-- cat("\nTest Error of Lasso Regression:",paste(round(lasso_err, digits = 2))) -->
<!-- # predict(mdl, s=lambda, type="coefficients") -->
<!-- ``` -->
```{r 6.11.a.2}
# predict function from chapter 6 labs
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}
```
> **Subset Selection _(Forward Selection)_**

```{r 6.11.a.forward-selection}
# forward selection
fit.fwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1)
(fwd.summary <- summary(fit.fwd))
fwd_err <- rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
  pred.fwd <- predict(fit.fwd, test, id=i)
  fwd_err[i] <- mean((test$crim - pred.fwd)^2)
}
plot(fwd_err, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
min <- which.min(fwd_err)
abline(v =min, col="red")
cat("\nMin. Test Error for Forward Selection is:",paste(round(fwd_err[min], digits = 2)))
```

<!-- ```{r} -->
<!-- par(mfrow=c(3,2)) -->

<!-- min.cp <- which.min(fwd.summary$cp) -->
<!-- plot(fwd.summary$cp, xlab="Number of Poly(X)", ylab="Forward Selection Cp", type="l") -->
<!-- points(min.cp, fwd.summary$cp[min.cp], col="red", pch=4, lwd=5) -->

<!-- min.cp <- which.min(bwd.summary$cp) -->
<!-- plot(bwd.summary$cp, xlab="Number of Poly(X)", ylab="Backward Selection Cp", type="l") -->
<!-- points(min.cp, bwd.summary$cp[min.cp], col="red", pch=4, lwd=5) -->

<!-- min.bic <- which.min(fwd.summary$bic) -->
<!-- plot(fwd.summary$bic, xlab="Number of Poly(X)", ylab="Forward Selection BIC", type="l") -->
<!-- points(min.bic, fwd.summary$bic[min.bic], col="red", pch=4, lwd=5) -->

<!-- min.bic <- which.min(bwd.summary$bic) -->
<!-- plot(bwd.summary$bic, xlab="Number of Poly(X)", ylab="Backward Selection BIC", type="l") -->
<!-- points(min.bic, bwd.summary$bic[min.bic], col="red", pch=4, lwd=5) -->

<!-- min.adjr2 <- which.max(fwd.summary$adjr2) -->
<!-- plot(fwd.summary$adjr2, xlab="Number of Poly(X)", ylab="Forward Selection Adj. R^2", type="l") -->
<!-- points(min.adjr2, fwd.summary$adjr2[min.adjr2], col="red", pch=4, lwd=5) -->

<!-- min.adjr2 <- which.max(bwd.summary$adjr2) -->
<!-- plot(bwd.summary$adjr2, xlab="Number of Poly(X)", ylab="Backward Selection Adj. R^2", type="l") -->
<!-- points(min.adjr2, bwd.summary$adjr2[min.adjr2], col="red", pch=4, lwd=5) -->
<!-- ``` -->

```{r 6.11.a.3, fig.align='center', fig.height = 5, fig.width = 5}
all_errs <- c(ridge_err, lasso_err, min(fwd_err))
names(all_errs) <- c("ridge", "lasso", "Forward")
barplot(all_errs, xlab = "Type of Regression", ylab = "MSE", 
        main = "Comparison of Regression Fit")
```

The regression methods _("Ridge", "Lasso", "Forward Selection")_ have approximately the same amount of error.

The **Subset Selection(Forward Selection) Regression** outperforms others by a slight margin.
Its **Test MSE** is: 69.55
```{r 6.11.a.4, fig.align='center', fig.height = 5, fig.width = 5}
plot(test$crim, xlab= "Actual Crime Rate", 
     (test$crim-pred.fwd), ylab = "Prediction Error",
     main = "Residuals")#,
     # xlim = c(0,100), ylim = c(0,100))
abline(h=0, col="red")
```


## _6.11.b_
From the above Test Set Errors, we can reasonably conclude that the 11-parameter Forward selection model is the
best fit to the Boston Dataset

## _6.11.c_
No because not all the predictors add much value to the model.
Adding more predictors makes the model more complex and computationally expensive.
Thus, If a predictor does not increase the amount of variance explained by the model significantly,
we can drop it.
In our case, We choose the Forward Selection model, which uses just 11 Predictors.

***
<a id="6.11"></a>

# EXERCISE 8.8:

```{r 8.8.setup, warning=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

knitr::opts_chunk$set(set.seed(7))

rm(list = ls())
require(tree)
require(ISLR)
require(randomForest)
attach(Carseats)
```

## _8.8.a_
```{r 8.8.a}
# 70-30 split
tr <- sample(1:nrow(Carseats), nrow(Carseats)*0.7)
carseats_train <- Carseats[tr, ]
carseats_test <- Carseats[-tr, ]

cat("Length of Carseats Dataset:",paste(nrow(Carseats)),"\n")
cat("Length of Train Dataset :",paste(nrow(carseats_train)),"\n")
cat("Length of Test Dataset  :",paste(nrow(carseats_test)))
```

## _8.8.b_
```{r 8.8.b.1}
carseats_tree <- tree(Sales~., data = carseats_train)
summary(carseats_tree)
```
```{r 8.8.b.2}
plot(carseats_tree)
text(carseats_tree, pretty = 0)
```
```{r 8.8.b.3}
tree_preds <- predict(carseats_tree, newdata = carseats_test)
tree_err <- mean((tree_preds - carseats_test$Sales)^2)
cat("Test MSE for tree is:",paste(tree_err))
```
## _8.8.c_
```{r 8.8.c.1}
carseats_cv <- cv.tree(carseats_tree)
plot(carseats_cv$size, xlab="Size of Tree",
     carseats_cv$dev, ylab="Deviation",
     main="Selecting Tree Size by Cross-Validation", type = "b")

tree_min <- which.min(carseats_cv$dev)
abline(v = carseats_cv$size[tree_min], col = "red")
points(carseats_cv$size[tree_min], carseats_cv$dev[tree_min], cex = 2, pch = 20)
```
Therefore, Cross-Validation suggests minimum Deviation at tree size of 8. 
Thus, Pruning the tree to size of 8
```{r 8.8.c.2}
carseats_pruned <- prune.tree(carseats_tree, best = carseats_cv$size[tree_min])
plot(carseats_pruned)
text(carseats_pruned, pretty = 0)
```
```{r 8.8.c.3}
pruned_preds <- predict(carseats_pruned, newdata = carseats_test)
pruned_err <- mean((pruned_preds - carseats_test$Sales)^2)
cat("Test MSE for Pruned tree is:",paste(pruned_err))
```
We observe that Test MSE has **increased** to 5.09 after pruning.

## _8.8.d_
For Bagging, Implementing Random Forest with each tree considering all parameters _(mtry = ncol(Carseats) -1)_

_**NOTE**: Subtracting 1 from ncol(Carseats) because one of the columns in the DataSet is the target column itself_
```{r 8.8.d.1}
carseats_bagging <- randomForest(Sales~.,
                                 data = carseats_train,
                                 mtry = ncol(Carseats)-1,
                                 ntree = 500, importance = TRUE)

preds_bagging <- predict(carseats_bagging, newdata = carseats_test)
bagging_err <- mean((preds_bagging - carseats_test$Sales)^2)
cat("Test MSE for Bagged Model is:",paste(bagging_err))
```
```{r 8.8.d.2}
x <- importance(carseats_bagging)
x[order(x[,"IncNodePurity"], decreasing = TRUE),]
```
From the above table, we see that **ShelveLoc** and **Price** are the most important predictors for _Sales_.

## _8.8.e_
For Random Forest, each tree will consider a subset of parameters
$$ m = \sqrt(p) $$
 Thus: _(mtry = floor(sqrt(ncol(Carseats) -1)))_
 
_**NOTE**: Subtracting 1 from ncol(Carseats) because one of the columns in the DataSet is the target column itself_
```{r 8.8.e.1}
carseats_randomfrst <- randomForest(Sales~.,
                                 data = carseats_train,
                                 mtry = floor(sqrt(ncol(Carseats) -1)),
                                 ntree = 500, importance = TRUE)

preds_randomfrst <- predict(carseats_randomfrst, newdata = carseats_test)
randomfrst_err <- mean((preds_randomfrst - carseats_test$Sales)^2)
cat("Test MSE for Random Forest Model is:",paste(randomfrst_err))
```
We observe that reducing _**m**_ from 10 _(Bagging)_ to 3 _(Random Forest)_ increases the Test MSE a little bit.

This increase in MSE can be explained by the fact that in Random Forest, 
unlike Bagging (wherein each tree is exposed to all the predictors), 
each tree is exposed to a subset of the predictors.

```{r 8.8.e.2}
x <- importance(carseats_randomfrst)
x[order(x[,"IncNodePurity"], decreasing = TRUE),]
```
From the above table, we see that **ShelveLoc** and **Price** are the still most important predictors for _Sales_.

***

<a id="8.8"></a>

# EXERCISE 8.11:

```{r 8.11.setup, warning=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

knitr::opts_chunk$set(set.seed(7))

rm(list = ls())
require(gbm)
require(class)
require(ISLR)
require(randomForest)
require(caret)
attach(Caravan)
```

## _8.11.a_
```{r 8.11.a}
#Converting the Caravan$Purchase to a 0/1 column so that Bernoulli works(GBM)
Caravan$Purchase <- ifelse(Caravan$Purchase == "Yes", 1,0)

# First 1000 training; rest test split
caravan_train <- Caravan[1:1000, ]
caravan_test <- Caravan[1001:nrow(Caravan), ]


cat("Length of Caravan Dataset:",paste(nrow(Caravan)),"\n")
cat("Length of Train Dataset :",paste(nrow(caravan_train)),"\n")
cat("Length of Test Dataset  :",paste(nrow(caravan_test)))
```

## _8.11.b_
```{r 8.11.b.1}
caravan_boost <- gbm(Purchase~.,
                     data = caravan_train,
                     n.trees = 1000,
                     shrinkage = 0.01,
                     distribution = "bernoulli")
summary(caravan_boost)
```
From the Table and Graph, we can conclude that the top **Most-Important Predictors** are:

- PPERSAUT
- MKOOPKLA
- MOPLHOOG

## _8.11.c_
> **Boosting**

```{r 8.11.c.boost}
boost_preds <- predict(caravan_boost, caravan_test, n.trees = 1000, type = "response")
boost_preds <- as.factor(ifelse(boost_preds > 0.2, 1, 0))
cm <- confusionMatrix(boost_preds, as.factor(caravan_test$Purchase), positive = "1")
cm$table
cat("[Boosting] Fraction of the people predicted to make a purchase who in fact make one:",
    paste(round(cm$byClass["Precision"], digits = 2)))
```
This value is also called the _**Precision**_

> **Logistic Regression**

```{r 8.11.c.logit}
caravan_logit <- glm(Purchase ~ ., data = caravan_train, family = "binomial")
logit_preds <- predict(caravan_logit, caravan_test, type = "response")
logit_preds <- as.factor(ifelse(logit_preds > 0.2, 1, 0))
cm <- confusionMatrix(logit_preds, as.factor(caravan_test$Purchase), positive = "1")
cm$table

cat("[Logistic Regression]Fraction of the people predicted to make a purchase who in fact make one:",
    paste(round(cm$byClass["Precision"], digits = 2)))
```
> **KNN**

```{r 8.11.c.knn}
kk <- seq(2,nrow(caravan_train)/2, by = 100)
out_precision_knn <- NULL

for(i in kk)
{
  i
  knn_preds=knn(caravan_train[,-86],caravan_test[,-86],caravan_train$Purchase,k=i)
  cm <- confusionMatrix(knn_preds, as.factor(caravan_test$Purchase), positive = "1")
  
  out_precision_knn <- c(out_precision_knn, cm$byClass["Precision"])
}
out_precision_knn <- as.data.frame(out_precision_knn)
rownames(out_precision_knn) <- kk
t(out_precision_knn)
```
Simply applying KNN does not work because the training data is highly skewed.
The following is the Summary of **Purchase** in Training Set:
```{r 8.11.c.1}
summary(as.factor(caravan_train$Purchase))
```
Thus, the model ends up predicting everything as class-0 or _Not Purchased_
Whole this achieves a High accuracy, the precision is low.

In order to improve the model, we probably need to _undersample_ 
the majority class in our training data.

**Summary:**
_**Boosting**_ outperforms _**Logistic Regression**_.
***

<a id="8.11"></a>